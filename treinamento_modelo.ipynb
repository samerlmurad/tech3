{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0735ffa",
   "metadata": {},
   "source": [
    "# LightGBM Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import databricks.automl_runtime\n",
    "\n",
    "target_col = \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41e387",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Create temp directory to download input data from MLflow\n",
    "input_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\n",
    "os.makedirs(input_temp_dir)\n",
    "\n",
    "\n",
    "# Download the artifact and read it into a pandas DataFrame\n",
    "input_data_path = mlflow.artifacts.download_artifacts(run_id=\"52c56cd289bb4bc48589caa91cd73033\", artifact_path=\"data\", dst_path=input_temp_dir)\n",
    "\n",
    "df_loaded = pd.read_parquet(os.path.join(input_data_path, \"training_data\"))\n",
    "# Delete the temp data\n",
    "shutil.rmtree(input_temp_dir)\n",
    "\n",
    "# Preview data\n",
    "display(df_loaded.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d5dad",
   "metadata": {},
   "source": [
    "### Select supported columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.automl_runtime.sklearn.column_selector import ColumnSelector\n",
    "supported_cols = [\"V3\", \"V10\", \"V25\", \"V19\", \"V6\", \"V18\", \"V21\", \"V13\", \"V1\", \"V2\", \"V12\", \"V16\", \"V15\", \"V5\", \"V22\", \"V14\", \"Amount\", \"V9\", \"V7\", \"Time\", \"V28\", \"V11\", \"V20\", \"V4\", \"V8\", \"V23\", \"V17\", \"V26\", \"V24\", \"V27\"]\n",
    "col_selector = ColumnSelector(supported_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8003066",
   "metadata": {},
   "source": [
    "## Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92876a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "num_imputers = []\n",
    "num_imputers.append((\"impute_mean\", SimpleImputer(), [\"Amount\", \"Time\", \"V1\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V2\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\"]))\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors='coerce'))),\n",
    "    (\"imputers\", ColumnTransformer(num_imputers)),\n",
    "    (\"standardizer\", StandardScaler()),\n",
    "])\n",
    "\n",
    "numerical_transformers = [(\"numerical\", numerical_pipeline, [\"V3\", \"V25\", \"V10\", \"V19\", \"V6\", \"V18\", \"V21\", \"V13\", \"V1\", \"V2\", \"V12\", \"V16\", \"V15\", \"V5\", \"V22\", \"V14\", \"Amount\", \"V9\", \"V7\", \"Time\", \"V28\", \"V11\", \"V20\", \"V4\", \"V8\", \"V23\", \"V17\", \"V26\", \"V24\", \"V27\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e626e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformers = numerical_transformers\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers, remainder=\"passthrough\", sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ddc55",
   "metadata": {},
   "source": [
    "## Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML completed train - validation - test split internally and used _automl_split_col_0000 to specify the set\n",
    "split_train_df = df_loaded.loc[df_loaded._automl_split_col_0000 == \"train\"]\n",
    "split_val_df = df_loaded.loc[df_loaded._automl_split_col_0000 == \"validate\"]\n",
    "split_test_df = df_loaded.loc[df_loaded._automl_split_col_0000 == \"test\"]\n",
    "\n",
    "# Separate target column from features and drop _automl_split_col_0000\n",
    "X_train = split_train_df.drop([target_col, \"_automl_split_col_0000\"], axis=1)\n",
    "y_train = split_train_df[target_col]\n",
    "\n",
    "X_val = split_val_df.drop([target_col, \"_automl_split_col_0000\"], axis=1)\n",
    "y_val = split_val_df[target_col]\n",
    "\n",
    "X_test = split_test_df.drop([target_col, \"_automl_split_col_0000\"], axis=1)\n",
    "y_test = split_test_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML balanced the data internally and use _automl_sample_weight_0000 to calibrate the probability distribution\n",
    "sample_weight_train = X_train.loc[:, \"_automl_sample_weight_0000\"].to_numpy()\n",
    "sample_weight_val = X_val.loc[:, \"_automl_sample_weight_0000\"].to_numpy()\n",
    "sample_weight_test = X_test.loc[:, \"_automl_sample_weight_0000\"].to_numpy()\n",
    "X_train = X_train.drop([\"_automl_sample_weight_0000\"], axis=1)\n",
    "X_val = X_val.drop([\"_automl_sample_weight_0000\"], axis=1)\n",
    "X_test = X_test.drop([\"_automl_sample_weight_0000\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c354105",
   "metadata": {},
   "source": [
    "## Train classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "help(LGBMClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c4152",
   "metadata": {},
   "source": [
    "### Define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import Model, infer_signature, ModelSignature\n",
    "from mlflow.pyfunc import PyFuncModel\n",
    "from mlflow import pyfunc\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, STATUS_OK, Trials\n",
    "\n",
    "# Create a separate pipeline to transform the validation dataset. This is used for early stopping.\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "pipeline_val = Pipeline([\n",
    "    (\"column_selector\", col_selector),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "])\n",
    "pipeline_val.fit(X_train, y_train)\n",
    "X_val_processed = pipeline_val.transform(X_val)\n",
    "\n",
    "def objective(params):\n",
    "  with mlflow.start_run(experiment_id=\"569257273296405\") as mlflow_run:\n",
    "    lgbmc_classifier = LGBMClassifier(**params)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"column_selector\", col_selector),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", lgbmc_classifier),\n",
    "    ])\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        silent=True)\n",
    "\n",
    "    model.fit(X_train, y_train, classifier__callbacks=[lightgbm.early_stopping(5), lightgbm.log_evaluation(0)], classifier__eval_set=[(X_val_processed,y_val)], classifier__sample_weight=sample_weight_train)\n",
    "\n",
    "    \n",
    "    # Log metrics for the training set\n",
    "    mlflow_model = Model()\n",
    "    pyfunc.add_to_model(mlflow_model, loader_module=\"mlflow.sklearn\")\n",
    "    pyfunc_model = PyFuncModel(model_meta=mlflow_model, model_impl=model)\n",
    "    training_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_train.assign(**{str(target_col):y_train}),\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"training_\" , \"pos_label\": 1, \"sample_weight\": sample_weight_train }\n",
    "    )\n",
    "    lgbmc_training_metrics = training_eval_result.metrics\n",
    "    # Log metrics for the validation set\n",
    "    val_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_val.assign(**{str(target_col):y_val}),\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"val_\" , \"pos_label\": 1, \"sample_weight\": sample_weight_val }\n",
    "    )\n",
    "    lgbmc_val_metrics = val_eval_result.metrics\n",
    "    # Log metrics for the test set\n",
    "    test_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_test.assign(**{str(target_col):y_test}),\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"test_\" , \"pos_label\": 1, \"sample_weight\": sample_weight_test }\n",
    "    )\n",
    "    lgbmc_test_metrics = test_eval_result.metrics\n",
    "\n",
    "    loss = -lgbmc_val_metrics[\"val_recall_score\"]\n",
    "\n",
    "    # Truncate metric key names so they can be displayed together\n",
    "    lgbmc_val_metrics = {k.replace(\"val_\", \"\"): v for k, v in lgbmc_val_metrics.items()}\n",
    "    lgbmc_test_metrics = {k.replace(\"test_\", \"\"): v for k, v in lgbmc_test_metrics.items()}\n",
    "\n",
    "    return {\n",
    "      \"loss\": loss,\n",
    "      \"status\": STATUS_OK,\n",
    "      \"val_metrics\": lgbmc_val_metrics,\n",
    "      \"test_metrics\": lgbmc_test_metrics,\n",
    "      \"model\": model,\n",
    "      \"run\": mlflow_run,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57980183",
   "metadata": {},
   "source": [
    "### Configure the hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15925932",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "  \"colsample_bytree\": 0.7963454674300969,\n",
    "  \"lambda_l1\": 2.9483149555562904,\n",
    "  \"lambda_l2\": 0.30921623674660015,\n",
    "  \"learning_rate\": 2.5815846093876433,\n",
    "  \"max_bin\": 198,\n",
    "  \"max_depth\": 12,\n",
    "  \"min_child_samples\": 80,\n",
    "  \"n_estimators\": 213,\n",
    "  \"num_leaves\": 89,\n",
    "  \"path_smooth\": 50.0976910988842,\n",
    "  \"subsample\": 0.7489002240479113,\n",
    "  \"random_state\": 645290059,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd835b89",
   "metadata": {},
   "source": [
    "### Run trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a470502",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "fmin(objective,\n",
    "     space=space,\n",
    "     algo=tpe.suggest,\n",
    "     max_evals=1,  # Increase this when widening the hyperparameter search space.\n",
    "     trials=trials)\n",
    "\n",
    "best_result = trials.best_trial[\"result\"]\n",
    "model = best_result[\"model\"]\n",
    "mlflow_run = best_result[\"run\"]\n",
    "\n",
    "display(\n",
    "  pd.DataFrame(\n",
    "    [best_result[\"val_metrics\"], best_result[\"test_metrics\"]],\n",
    "    index=[\"validation\", \"test\"]))\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f027f8",
   "metadata": {},
   "source": [
    "### Patch pandas version in logged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca409967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import yaml\n",
    "\n",
    "run_id = mlflow_run.info.run_id\n",
    "\n",
    "# Set up a local dir for downloading the artifacts.\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Fix conda.yaml\n",
    "conda_file_path = mlflow.artifacts.download_artifacts(artifact_uri=f\"runs:/{run_id}/model/conda.yaml\", dst_path=tmp_dir)\n",
    "with open(conda_file_path) as f:\n",
    "  conda_libs = yaml.load(f, Loader=yaml.FullLoader)\n",
    "pandas_lib_exists = any([lib.startswith(\"pandas==\") for lib in conda_libs[\"dependencies\"][-1][\"pip\"]])\n",
    "if not pandas_lib_exists:\n",
    "  print(\"Adding pandas dependency to conda.yaml\")\n",
    "  conda_libs[\"dependencies\"][-1][\"pip\"].append(f\"pandas=={pd.__version__}\")\n",
    "\n",
    "  with open(f\"{tmp_dir}/conda.yaml\", \"w\") as f:\n",
    "    f.write(yaml.dump(conda_libs))\n",
    "  client.log_artifact(run_id=run_id, local_path=conda_file_path, artifact_path=\"model\")\n",
    "\n",
    "# Fix requirements.txt\n",
    "venv_file_path = mlflow.artifacts.download_artifacts(artifact_uri=f\"runs:/{run_id}/model/requirements.txt\", dst_path=tmp_dir)\n",
    "with open(venv_file_path) as f:\n",
    "  venv_libs = f.readlines()\n",
    "venv_libs = [lib.strip() for lib in venv_libs]\n",
    "pandas_lib_exists = any([lib.startswith(\"pandas==\") for lib in venv_libs])\n",
    "if not pandas_lib_exists:\n",
    "  print(\"Adding pandas dependency to requirements.txt\")\n",
    "  venv_libs.append(f\"pandas=={pd.__version__}\")\n",
    "\n",
    "  with open(f\"{tmp_dir}/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(venv_libs))\n",
    "  client.log_artifact(run_id=run_id, local_path=venv_file_path, artifact_path=\"model\")\n",
    "\n",
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc67829",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to True and re-run the notebook to see the SHAP plots\n",
    "shap_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e817426",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_enabled:\n",
    "    mlflow.autolog(disable=True)\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "    from shap import KernelExplainer, summary_plot\n",
    "    # Sample background data for SHAP Explainer. Increase the sample size to reduce variance.\n",
    "    train_sample = X_train.sample(n=min(100, X_train.shape[0]), random_state=645290059)\n",
    "\n",
    "    # Sample some rows from the validation set to explain. Increase the sample size for more thorough results.\n",
    "    example = X_val.sample(n=min(100, X_val.shape[0]), random_state=645290059)\n",
    "\n",
    "    # Use Kernel SHAP to explain feature importance on the sampled rows from the validation set.\n",
    "    predict = lambda x: model.predict(pd.DataFrame(x, columns=X_train.columns))\n",
    "    explainer = KernelExplainer(predict, train_sample, link=\"identity\")\n",
    "    shap_values = explainer.shap_values(example, l1_reg=False, nsamples=500)\n",
    "    summary_plot(shap_values, example, class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4878153",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_uri for the generated model\n",
    "print(f\"runs:/{ mlflow_run.info.run_id }/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf62ad",
   "metadata": {},
   "source": [
    "## Confusion matrix, ROC and Precision-Recall curves for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b48365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the link to see the MLflow run page\n",
    "displayHTML(f\"<a href=#mlflow/experiments/569257273296405/runs/{ mlflow_run.info.run_id }/artifactPath/model> Link to model run page </a>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87357b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create temp directory to download MLflow model artifact\n",
    "eval_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\n",
    "os.makedirs(eval_temp_dir, exist_ok=True)\n",
    "\n",
    "# Download the artifact\n",
    "eval_path = mlflow.artifacts.download_artifacts(run_id=mlflow_run.info.run_id, dst_path=eval_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_roc_curve_path = os.path.join(eval_path, \"val_roc_curve_plot.png\")\n",
    "display(Image(filename=eval_roc_curve_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pr_curve_path = os.path.join(eval_path, \"val_precision_recall_curve_plot.png\")\n",
    "display(Image(filename=eval_pr_curve_path))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
